# Plan d'Impl√©mentation - Configuration LLM via MCP Config

## üìã Objectif
Permettre aux utilisateurs de configurer leur mod√®le LLM et cl√© API directement dans la configuration MCP de Windsurf, sans avoir √† modifier les fichiers .env ou le code source.

## üéØ Configuration Cible
```json
{
  "collegue": {
    "serverUrl": "http://localhost:8088/mcp/",
    "LLM_MODEL": "google/gemini-2.0-flash-exp:free",
    "LLM_API_KEY": "sk-or-v1-xxxxx"
  }
}
```

## üîÑ Flux de Donn√©es
```mermaid
graph LR
    A[mcp_config.json] --> B[FastMCP Server]
    B --> C[app.py]
    C --> D[config.py]
    D --> E[ToolLLMManager]
    E --> F[OpenRouter API]
```

## üìù Plan d'Impl√©mentation

### Phase 1: Modification de la Configuration (config.py)
- [ ] Ajouter support pour les variables MCP dans `Config`
- [ ] Cr√©er une m√©thode `update_from_mcp()` pour surcharger les valeurs
- [ ] Garder la compatibilit√© avec les variables d'environnement

### Phase 2: R√©cup√©ration des Param√®tres MCP (app.py)
- [ ] Intercepter les param√®tres MCP lors de l'initialisation
- [ ] Extraire `LLM_MODEL` et `LLM_API_KEY` des m√©tadonn√©es
- [ ] Passer ces valeurs √† la configuration

### Phase 3: Adaptation du ToolLLMManager
- [ ] Modifier l'initialisation pour accepter des param√®tres dynamiques
- [ ] Permettre la mise √† jour √† chaud de la configuration
- [ ] G√©rer les erreurs si les param√®tres sont invalides

### Phase 4: Documentation et Validation
- [ ] Documenter le nouveau format de configuration
- [ ] Cr√©er des exemples pour diff√©rents providers
- [ ] Ajouter des tests unitaires

## üõ†Ô∏è Modifications Techniques

### 1. config.py
```python
class Config(BaseSettings):
    # Existant...
    
    # Nouvelles propri√©t√©s pour MCP
    _mcp_llm_model: Optional[str] = None
    _mcp_llm_api_key: Optional[str] = None
    
    def update_from_mcp(self, mcp_params: dict):
        """Met √† jour la configuration avec les param√®tres MCP"""
        if "LLM_MODEL" in mcp_params:
            self._mcp_llm_model = mcp_params["LLM_MODEL"]
        if "LLM_API_KEY" in mcp_params:
            self._mcp_llm_api_key = mcp_params["LLM_API_KEY"]
    
    @property
    def llm_model(self) -> str:
        """Retourne le mod√®le LLM (priorit√©: MCP > env > default)"""
        return self._mcp_llm_model or self.LLM_MODEL
    
    @property
    def llm_api_key(self) -> str:
        """Retourne la cl√© API (priorit√©: MCP > env > default)"""
        return self._mcp_llm_api_key or self.LLM_API_KEY
```

### 2. app.py
```python
@app.on_startup
async def on_startup():
    # R√©cup√©rer les param√®tres MCP si disponibles
    mcp_params = get_mcp_parameters()  # √Ä impl√©menter
    if mcp_params:
        config.update_from_mcp(mcp_params)
    
    # R√©initialiser le ToolLLMManager avec la nouvelle config
    app_state["llm_manager"] = ToolLLMManager(
        api_key=config.llm_api_key,
        model=config.llm_model
    )
```

### 3. Extraction des Param√®tres MCP
FastMCP transmet les param√®tres personnalis√©s via les m√©tadonn√©es de transport. Il faut:
- Intercepter ces m√©tadonn√©es lors de l'initialisation
- Les stocker dans l'√©tat de l'application
- Les utiliser pour configurer le LLM

## üîç Validation et Tests

### Tests de Configuration
1. **Sans param√®tres MCP** : Doit utiliser les variables d'environnement
2. **Avec param√®tres MCP** : Doit surcharger les variables d'environnement
3. **Param√®tres invalides** : Doit g√©rer gracieusement les erreurs

### Mod√®les OpenRouter Populaires
```json
// Gratuits
"google/gemini-2.0-flash-exp:free"
"meta-llama/llama-3.2-11b-vision-instruct:free"

// √âconomiques
"openai/gpt-4o-mini"
"anthropic/claude-3.5-haiku"

// Performants
"openai/gpt-4o"
"anthropic/claude-3.5-sonnet"
```

## üìö Documentation Utilisateur

### Configuration Minimale
```json
{
  "collegue": {
    "serverUrl": "http://localhost:8088/mcp/"
  }
}
```
Utilise les variables d'environnement ou les valeurs par d√©faut.

### Configuration Compl√®te
```json
{
  "collegue": {
    "serverUrl": "http://localhost:8088/mcp/",
    "LLM_MODEL": "openai/gpt-4o-mini",
    "LLM_API_KEY": "sk-or-v1-votre-cle-api"
  }
}
```
Surcharge toutes les configurations par d√©faut.

## ‚ö†Ô∏è Consid√©rations de S√©curit√©

1. **Ne jamais logger la cl√© API** en clair
2. **Valider le format** de la cl√© API (sk-or-v1-*)
3. **G√©rer les erreurs** d'authentification gracieusement
4. **Documenter** que la cl√© est visible dans le fichier de config

## üöÄ Avantages

- ‚úÖ **Simplicit√©** : Configuration directe dans Windsurf
- ‚úÖ **Flexibilit√©** : Chaque utilisateur peut choisir son mod√®le
- ‚úÖ **S√©curit√©** : Pas besoin de modifier les fichiers sources
- ‚úÖ **Compatibilit√©** : Garde le support des variables d'environnement

## üìÖ Timeline

1. **Phase 1-2** : 30 minutes - Modification config et app.py
2. **Phase 3** : 15 minutes - Adaptation ToolLLMManager
3. **Phase 4** : 15 minutes - Tests et documentation
4. **Total** : ~1 heure

## üéØ Crit√®res de Succ√®s

- [ ] Les utilisateurs peuvent configurer leur mod√®le LLM depuis mcp_config.json
- [ ] Les utilisateurs peuvent configurer leur cl√© API depuis mcp_config.json
- [ ] La configuration par d√©faut continue de fonctionner
- [ ] Les variables d'environnement restent support√©es
- [ ] La documentation est claire et compl√®te
