# Guide de Configuration MCP pour Windsurf

## Probl√®mes Identifi√©s et Solutions Implement√©es

### ‚úÖ 1. Port Corrig√©
- **Probl√®me** : Le port √©tait configur√© sur 8080 mais Docker expose sur 8088
- **Solution** : Tous les scripts utilisent maintenant `http://localhost:8088/mcp/`

### ‚úÖ 2. Headers HTTP Corrig√©s
- **Probl√®me** : Le serveur MCP exige `Accept: application/json, text/event-stream`
- **Solution** : Headers correctement configur√©s dans tous les tests

### ‚úÖ 3. Endpoint de Sant√© Ajout√©
- **Probl√®me** : L'endpoint `/_health` n'√©tait pas accessible via nginx
- **Solution** : Configuration nginx mise √† jour avec endpoint sp√©cifique

### ‚úÖ 4. Initialisation MCP Fonctionnelle
- **Probl√®me** : Param√®tres d'initialisation incorrects
- **Solution** : Structure JSON-RPC 2.0 correcte impl√©ment√©e

### ‚ö†Ô∏è 5. Gestion de Session ID
- **Statut** : Partiellement r√©solu
- **D√©tail** : L'ID de session est cr√©√© c√¥t√© serveur mais la transmission client n'est pas correcte

## Configuration Recommand√©e pour Windsurf

```json
{
  "collegue": {
    "serverUrl": "http://localhost:8088/mcp/"
  }
}
```

## Tests de Validation

### Test Basique
```bash
python3 test_mcp_connection.py
```

### Test Avanc√© avec Session
```bash
python3 test_mcp_session.py
```

## R√©sultats des Tests

‚úÖ **Serveur Accessible** : Port 8088 fonctionne  
‚úÖ **Initialisation MCP** : Status 200, protocol version 2024-11-05  
‚úÖ **Cr√©ation Session** : ID g√©n√©r√© c√¥t√© serveur  
‚ùå **Utilisation Session** : Transmission session ID √† r√©soudre  

## Prochaines √âtapes

1. **Pour le d√©veloppement** : Le serveur fonctionne et peut √™tre utilis√© avec un client MCP compatible
2. **Pour Windsurf** : Utilisez la configuration recommand√©e ci-dessus
3. **Pour le debugging** : Surveillez les logs avec `docker compose logs collegue-app`

## Logs de Session Exemple

```
INFO:mcp.server.streamable_http_manager:Created new transport with session ID: c88d16d3556a421e89f3f4f6ce92eecb
```

## üöÄ Configuration Rapide avec Windsurf

### Configuration de base (utilise les variables d'environnement)
```json
{
  "collegue": {
    "serverUrl": "http://localhost:8088/mcp/"
  }
}
```

### Configuration personnalis√©e avec mod√®le et cl√© API
```json
{
  "collegue": {
    "serverUrl": "http://localhost:8088/mcp/",
    "LLM_MODEL": "gemini-3-flash-preview",
    "LLM_API_KEY": "AIzaSy-votre-cle-api-gemini"
  }
}
```

## üìã Priorit√© de Configuration

Le syst√®me utilise la priorit√© suivante pour les param√®tres :
1. **MCP Config** (mcp_config.json dans Windsurf) - **Priorit√© maximale**
2. **Variables d'environnement** (.env ou export)
3. **Valeurs par d√©faut** du code

### Exemple de priorit√©
- Si `LLM_MODEL` est d√©fini dans mcp_config.json ‚Üí utilise cette valeur
- Sinon, si `LLM_MODEL` est dans .env ‚Üí utilise la valeur .env
- Sinon ‚Üí utilise la valeur par d√©faut "gemini-3-flash-preview"

## üîß Installation et Configuration

### Pr√©requis
- Python 3.10+
- Docker et Docker Compose
- Cl√© API Google Gemini ([obtenir une cl√©](https://aistudio.google.com/apikey))

### Installation locale
```bash
# Cloner le repository
git clone https://github.com/your-org/collegue-mcp.git
cd collegue-mcp

# Cr√©er l'environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Installer les d√©pendances
pip install -r requirements.txt
```

### Configuration avec fichier .env (optionnel)
Cr√©ez un fichier `.env` √† la racine :
```env
# Configuration de base (sera surcharg√©e par MCP si d√©fini)
LLM_API_KEY=AIzaSy-your-api-key-here
LLM_MODEL=gemini-3-flash-preview

# Configuration serveur
HOST=0.0.0.0
PORT=4121

# OAuth (optionnel)
OAUTH_ENABLED=false
```

## üéØ Mod√®les Google Gemini Disponibles

### Mod√®les Gratuits
```json
"LLM_MODEL": "gemini-3-flash-preview"
"LLM_MODEL": "gemini-2.5-flash"
"LLM_MODEL": "gemini-2.5-flash-lite"
```

### Mod√®les √âconomiques
```json
"LLM_MODEL": "gemini-2.5-flash"        // Recommand√© - Rapide et √©conomique
"LLM_MODEL": "gemini-2.5-pro"         // Haute performance
```

### Mod√®les Performants
```json
"LLM_MODEL": "gemini-2.5-pro"           // Gemini Pro optimis√©
"LLM_MODEL": "gemini-3-flash"           // Gemini 3 Flash
```

## üê≥ Utilisation avec Docker

### D√©marrage rapide
```bash
# Configuration minimale (utilise .env)
docker-compose up -d

# Avec param√®tres MCP personnalis√©s
MCP_LLM_MODEL="gemini-3-flash-preview" MCP_LLM_API_KEY="AIzaSy-xxx" docker-compose up -d
```

### Services disponibles
- **collegue-app** : Serveur MCP principal (port 4121)
- **nginx** : Proxy HTTP (port 8088)
- **keycloak** : OAuth (port 4123) - optionnel

## üîç Test de Configuration

### V√©rifier la configuration active
```bash
# Health check
curl http://localhost:8088/health

# Voir les logs pour confirmer le mod√®le utilis√©
docker-compose logs collegue-app | grep "mod√®le"
```

### Test avec le client Python
```python
from collegue.client import CollegueClient

client = CollegueClient(base_url="http://localhost:8088")

# Tester la g√©n√©ration de code
result = await client.generate_code(
    language="python",
    description="Une fonction pour calculer fibonacci"
)
print(result)
```

## ‚ö†Ô∏è S√©curit√©

### Bonnes Pratiques
1. **Ne jamais commit** votre cl√© API dans git
2. **Utiliser .gitignore** pour exclure .env et mcp_config.json
3. **Rotation r√©guli√®re** des cl√©s API
4. **Limiter les permissions** de la cl√© API si possible

### Format de Cl√© API Google Gemini
Les cl√©s Google Gemini commencent toujours par `AIzaSy`

## üêõ D√©pannage

### Erreur : "La cl√© API LLM n'est pas configur√©e"
**Solutions :**
1. V√©rifiez votre mcp_config.json dans Windsurf
2. V√©rifiez le fichier .env
3. Format correct : `AIzaSy-xxxxx`

### Erreur : "Mod√®le non disponible"
**Solutions :**
1. V√©rifiez que le mod√®le existe sur [Google AI Studio](https://aistudio.google.com)
2. Certains mod√®les n√©cessitent des cr√©dits
3. Utilisez un mod√®le gratuit pour tester

### Logs et Debugging
```bash
# Voir tous les logs
docker-compose logs -f

# Logs avec le mod√®le utilis√©
docker-compose logs collegue-app | grep "ToolLLMManager"

# Mode debug
DEBUG=true docker-compose up
```

## üìö Exemples de Configuration

### Configuration D√©veloppement (Gratuite)
```json
{
  "collegue": {
    "serverUrl": "http://localhost:8088/mcp/",
    "LLM_MODEL": "gemini-3-flash-preview",
    "LLM_API_KEY": "AIzaSy-dev-key"
  }
}
```

### Configuration Production (Performante)
```json
{
  "collegue": {
    "serverUrl": "https://collegue.example.com/mcp/",
    "LLM_MODEL": "gemini-2.5-pro",
    "LLM_API_KEY": "AIzaSy-prod-key"
  }
}
```

### Configuration Test (√âconomique)
```json
{
  "collegue": {
    "serverUrl": "http://localhost:8088/mcp/",
    "LLM_MODEL": "gemini-2.5-flash",
    "LLM_API_KEY": "AIzaSy-test-key"
  }
}
```

## üí° Tips et Astuces

1. **Commencez avec un mod√®le gratuit** pour tester la configuration
2. **Utilisez gemini-3-flash-preview** pour un bon √©quilibre performance/co√ªt
3. **Surveillez votre usage** sur [Google Cloud Console](https://console.cloud.google.com/billing)
4. **Configurez des limites** de d√©penses dans Google Cloud
5. **Testez diff√©rents mod√®les** pour trouver le meilleur pour votre usage

## üìû Support

Pour toute question :
- Consultez le [README principal](../README.md)
- Ouvrez une issue sur GitHub
- Consultez la [documentation Google Gemini API](https://ai.google.dev/gemini-api/docs)

Le serveur Coll√®gue MCP est maintenant pr√™t pour utilisation !
