# Guide de Configuration MCP pour Windsurf

## Probl√®mes Identifi√©s et Solutions Implement√©es

### ‚úÖ 1. Port Corrig√©
- **Probl√®me** : Le port √©tait configur√© sur 8080 mais Docker expose sur 8088
- **Solution** : Tous les scripts utilisent maintenant `http://localhost:8088/mcp/`

### ‚úÖ 2. Headers HTTP Corrig√©s
- **Probl√®me** : Le serveur MCP exige `Accept: application/json, text/event-stream`
- **Solution** : Headers correctement configur√©s dans tous les tests

### ‚úÖ 3. Endpoint de Sant√© Ajout√©
- **Probl√®me** : L'endpoint `/_health` n'√©tait pas accessible via nginx
- **Solution** : Configuration nginx mise √† jour avec endpoint sp√©cifique

### ‚úÖ 4. Initialisation MCP Fonctionnelle
- **Probl√®me** : Param√®tres d'initialisation incorrects
- **Solution** : Structure JSON-RPC 2.0 correcte impl√©ment√©e

### ‚ö†Ô∏è 5. Gestion de Session ID
- **Statut** : Partiellement r√©solu
- **D√©tail** : L'ID de session est cr√©√© c√¥t√© serveur mais la transmission client n'est pas correcte

## Configuration Recommand√©e pour Windsurf

```json
{
  "collegue": {
    "serverUrl": "http://localhost:8088/mcp/"
  }
}
```

## Tests de Validation

### Test Basique
```bash
python3 test_mcp_connection.py
```

### Test Avanc√© avec Session
```bash
python3 test_mcp_session.py
```

## R√©sultats des Tests

‚úÖ **Serveur Accessible** : Port 8088 fonctionne  
‚úÖ **Initialisation MCP** : Status 200, protocol version 2024-11-05  
‚úÖ **Cr√©ation Session** : ID g√©n√©r√© c√¥t√© serveur  
‚ùå **Utilisation Session** : Transmission session ID √† r√©soudre  

## Prochaines √âtapes

1. **Pour le d√©veloppement** : Le serveur fonctionne et peut √™tre utilis√© avec un client MCP compatible
2. **Pour Windsurf** : Utilisez la configuration recommand√©e ci-dessus
3. **Pour le debugging** : Surveillez les logs avec `docker compose logs collegue-app`

## Logs de Session Exemple

```
INFO:mcp.server.streamable_http_manager:Created new transport with session ID: c88d16d3556a421e89f3f4f6ce92eecb
```

## üöÄ Configuration Rapide avec Windsurf

### Configuration de base (utilise les variables d'environnement)
```json
{
  "collegue": {
    "serverUrl": "http://localhost:8088/mcp/"
  }
}
```

### Configuration personnalis√©e avec mod√®le et cl√© API
```json
{
  "collegue": {
    "serverUrl": "http://localhost:8088/mcp/",
    "LLM_MODEL": "openai/gpt-4o-mini",
    "LLM_API_KEY": "sk-or-v1-votre-cle-api-openrouter"
  }
}
```

## üìã Priorit√© de Configuration

Le syst√®me utilise la priorit√© suivante pour les param√®tres :
1. **MCP Config** (mcp_config.json dans Windsurf) - **Priorit√© maximale**
2. **Variables d'environnement** (.env ou export)
3. **Valeurs par d√©faut** du code

### Exemple de priorit√©
- Si `LLM_MODEL` est d√©fini dans mcp_config.json ‚Üí utilise cette valeur
- Sinon, si `LLM_MODEL` est dans .env ‚Üí utilise la valeur .env
- Sinon ‚Üí utilise la valeur par d√©faut "openai/gpt-5-mini"

## üîß Installation et Configuration

### Pr√©requis
- Python 3.10+
- Docker et Docker Compose
- Cl√© API OpenRouter ([obtenir une cl√©](https://openrouter.ai/keys))

### Installation locale
```bash
# Cloner le repository
git clone https://github.com/your-org/collegue-mcp.git
cd collegue-mcp

# Cr√©er l'environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Installer les d√©pendances
pip install -r requirements.txt
```

### Configuration avec fichier .env (optionnel)
Cr√©ez un fichier `.env` √† la racine :
```env
# Configuration de base (sera surcharg√©e par MCP si d√©fini)
LLM_API_KEY=sk-or-v1-your-api-key-here
LLM_MODEL=openai/gpt-4o-mini

# Configuration serveur
HOST=0.0.0.0
PORT=4121

# OAuth (optionnel)
OAUTH_ENABLED=false
```

## üéØ Mod√®les OpenRouter Disponibles

### Mod√®les Gratuits
```json
"LLM_MODEL": "google/gemini-2.0-flash-exp:free"
"LLM_MODEL": "meta-llama/llama-3.2-11b-vision-instruct:free"
"LLM_MODEL": "google/gemini-flash-1.5-8b:free"
```

### Mod√®les √âconomiques
```json
"LLM_MODEL": "openai/gpt-4o-mini"        // Recommand√© - Rapide et √©conomique
"LLM_MODEL": "anthropic/claude-3.5-haiku" // Tr√®s rapide
"LLM_MODEL": "deepseek/deepseek-chat"     // Bon rapport qualit√©/prix
```

### Mod√®les Performants
```json
"LLM_MODEL": "openai/gpt-4o"              // GPT-4 optimis√©
"LLM_MODEL": "anthropic/claude-3.5-sonnet" // Claude performant
"LLM_MODEL": "google/gemini-pro-1.5"       // Gemini haut de gamme
```

## üê≥ Utilisation avec Docker

### D√©marrage rapide
```bash
# Configuration minimale (utilise .env)
docker-compose up -d

# Avec param√®tres MCP personnalis√©s
MCP_LLM_MODEL="openai/gpt-4o" MCP_LLM_API_KEY="sk-or-v1-xxx" docker-compose up -d
```

### Services disponibles
- **collegue-app** : Serveur MCP principal (port 4121)
- **nginx** : Proxy HTTP (port 8088)
- **keycloak** : OAuth (port 4123) - optionnel

## üîç Test de Configuration

### V√©rifier la configuration active
```bash
# Health check
curl http://localhost:8088/health

# Voir les logs pour confirmer le mod√®le utilis√©
docker-compose logs collegue-app | grep "mod√®le"
```

### Test avec le client Python
```python
from collegue.client import CollegueClient

client = CollegueClient(base_url="http://localhost:8088")

# Tester la g√©n√©ration de code
result = await client.generate_code(
    language="python",
    description="Une fonction pour calculer fibonacci"
)
print(result)
```

## ‚ö†Ô∏è S√©curit√©

### Bonnes Pratiques
1. **Ne jamais commit** votre cl√© API dans git
2. **Utiliser .gitignore** pour exclure .env et mcp_config.json
3. **Rotation r√©guli√®re** des cl√©s API
4. **Limiter les permissions** de la cl√© API si possible

### Format de Cl√© API OpenRouter
Les cl√©s OpenRouter commencent toujours par `sk-or-v1-`

## üêõ D√©pannage

### Erreur : "La cl√© API LLM n'est pas configur√©e"
**Solutions :**
1. V√©rifiez votre mcp_config.json dans Windsurf
2. V√©rifiez le fichier .env
3. Format correct : `sk-or-v1-xxxxx`

### Erreur : "Mod√®le non disponible"
**Solutions :**
1. V√©rifiez que le mod√®le existe sur [OpenRouter](https://openrouter.ai/models)
2. Certains mod√®les n√©cessitent des cr√©dits
3. Utilisez un mod√®le gratuit pour tester

### Logs et Debugging
```bash
# Voir tous les logs
docker-compose logs -f

# Logs avec le mod√®le utilis√©
docker-compose logs collegue-app | grep "ToolLLMManager"

# Mode debug
DEBUG=true docker-compose up
```

## üìö Exemples de Configuration

### Configuration D√©veloppement (Gratuite)
```json
{
  "collegue": {
    "serverUrl": "http://localhost:8088/mcp/",
    "LLM_MODEL": "google/gemini-2.0-flash-exp:free",
    "LLM_API_KEY": "sk-or-v1-dev-key"
  }
}
```

### Configuration Production (Performante)
```json
{
  "collegue": {
    "serverUrl": "https://collegue.example.com/mcp/",
    "LLM_MODEL": "openai/gpt-4o",
    "LLM_API_KEY": "sk-or-v1-prod-key"
  }
}
```

### Configuration Test (√âconomique)
```json
{
  "collegue": {
    "serverUrl": "http://localhost:8088/mcp/",
    "LLM_MODEL": "openai/gpt-4o-mini",
    "LLM_API_KEY": "sk-or-v1-test-key"
  }
}
```

## üí° Tips et Astuces

1. **Commencez avec un mod√®le gratuit** pour tester la configuration
2. **Utilisez gpt-4o-mini** pour un bon √©quilibre performance/co√ªt
3. **Surveillez votre usage** sur [OpenRouter Dashboard](https://openrouter.ai/dashboard)
4. **Configurez des limites** de d√©penses dans OpenRouter
5. **Testez diff√©rents mod√®les** pour trouver le meilleur pour votre usage

## üìû Support

Pour toute question :
- Consultez le [README principal](../README.md)
- Ouvrez une issue sur GitHub
- Consultez la [documentation OpenRouter](https://openrouter.ai/docs)

Le serveur Coll√®gue MCP est maintenant pr√™t pour utilisation !
